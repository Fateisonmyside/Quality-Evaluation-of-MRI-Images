{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "history_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-6Z_z0X7ag3"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import nibabel as nib\n",
        "from nibabel.testing import data_path\n",
        "import pandas as pd\n",
        "import nibabel as nib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pickle\n",
        "from scipy import ndimage\n",
        "import random\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "V0HJVh-vIe3V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "793e0330-f70b-4dd8-ca9f-3a275b9c188e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path='/content/gdrive/MyDrive/BET_BSE_DATA/'"
      ],
      "metadata": {
        "id": "ixo6joSyGnmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = pd.read_csv(path + \"Label_file.csv\")\n",
        "labelsData = labels.iloc[: , :-1]\n",
        "labelsData['Filename']=labelsData['Filename']+'.gz'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDyP4bnaMn8G",
        "outputId": "ec70a6c3-2add-46dc-b8e6-dd9965e9dc20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import ndimage\n",
        "\n",
        "def read_nifti_file(filepath):\n",
        "    scan = nib.load(filepath).get_fdata()\n",
        "    return scan\n",
        "\n",
        "\n",
        "def normalize(volume):\n",
        "    min = -1000\n",
        "    max = 400\n",
        "    volume[volume < min] = min\n",
        "    volume[volume > max] = max\n",
        "    volume = (volume - min) / (max - min)\n",
        "    return volume.astype(\"float32\")\n",
        "\n",
        "\n",
        "def resize_volume(img):\n",
        "    desired_depth = 64\n",
        "    desired_width = 128\n",
        "    desired_height = 128\n",
        "    current_depth = img.shape[-1]\n",
        "    current_width = img.shape[0]\n",
        "    current_height = img.shape[1]\n",
        "    depth = current_depth / desired_depth\n",
        "    width = current_width / desired_width\n",
        "    height = current_height / desired_height\n",
        "    depth_factor = 1 / depth\n",
        "    width_factor = 1 / width\n",
        "    height_factor = 1 / height\n",
        "    img = ndimage.rotate(img, 90, reshape=False)\n",
        "    img = ndimage.zoom(img, (width_factor, height_factor, depth_factor), order=1)\n",
        "    return img\n",
        "\n",
        "\n",
        "def process_scan(path):\n",
        "    volume = read_nifti_file(path)\n",
        "    volume = normalize(volume)\n",
        "    volume = resize_volume(volume)\n",
        "    return volume"
      ],
      "metadata": {
        "id": "TkN4x22OM3pn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np_data = np.array([process_scan(path + 'files/' + filepath) for filepath in labelsData['Filename']])"
      ],
      "metadata": {
        "id": "gbCeLeJxNEE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np_data[:918, :]\n",
        "X_val = np_data[918:, :]"
      ],
      "metadata": {
        "id": "d6ExfkJuZgEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels['Recognizable-Facial-Feature'] = labels['Recognizable-Facial-Feature'].replace({'Yes': 1, 'No': 0})\n",
        "y_train = labels['Recognizable-Facial-Feature'][:918]\n",
        "y_val = labels['Recognizable-Facial-Feature'][918:]"
      ],
      "metadata": {
        "id": "PSWSGrotabU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def rotate(volume):\n",
        "    def scipy_rotate(volume):\n",
        "        angles = [-20, 20]\n",
        "        angle = random.choice(angles)\n",
        "        volume = ndimage.rotate(volume, angle, reshape=False)\n",
        "        volume[volume < 0] = 0\n",
        "        volume[volume > 1] = 1\n",
        "        return volume\n",
        "    augmented_volume = tf.numpy_function(scipy_rotate, [volume], tf.float32)\n",
        "    return augmented_volume\n",
        "\n",
        "\n",
        "def train_preprocessing(volume, label):\n",
        "    volume = rotate(volume)\n",
        "    volume = tf.expand_dims(volume, axis=3)\n",
        "    return volume, label\n",
        "\n",
        "\n",
        "def test_preprocessing(volume, label):\n",
        "    volume = tf.expand_dims(volume, axis=3)\n",
        "    return volume, label"
      ],
      "metadata": {
        "id": "4onbcm0AoQ_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "test_loader = tf.data.Dataset.from_tensor_slices((X_val, y_val))"
      ],
      "metadata": {
        "id": "Lr_SvA2Upych"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 8\n",
        "train_dataset = (\n",
        "    train_loader.shuffle(len(X_train))\n",
        "    .map(train_preprocessing)\n",
        "    .batch(batch_size)\n",
        "    .prefetch(2)\n",
        ")\n",
        "test_dataset = (\n",
        "    test_loader.shuffle(len(X_val))\n",
        "    .map(test_preprocessing)\n",
        "    .batch(batch_size)\n",
        "    .prefetch(2)\n",
        ")"
      ],
      "metadata": {
        "id": "1urj2nrZp_Gt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(width=128, height=128, depth=64):\n",
        "    inputs = keras.Input((width, height, depth, 1))\n",
        "\n",
        "    x = layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(inputs)\n",
        "    x = layers.MaxPool3D(pool_size=2)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "    x = layers.MaxPool3D(pool_size=2)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.Conv3D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "    x = layers.MaxPool3D(pool_size=2)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.Conv3D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "    x = layers.MaxPool3D(pool_size=2)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.GlobalAveragePooling3D()(x)\n",
        "    x = layers.Dense(units=512, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "\n",
        "    outputs = layers.Dense(units=1, activation=\"sigmoid\")(x)\n",
        "\n",
        "    model = keras.Model(inputs, outputs, name=\"3dcnn\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "Qk98U5SBsMDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model(width=128, height=128, depth=64)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AM12eyDvKPA",
        "outputId": "87fb3da5-fac0-4a40-c171-f7b10e7e5899"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"3dcnn\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 128, 128, 64, 1)  0         \n",
            "                             ]                                   \n",
            "                                                                 \n",
            " conv3d (Conv3D)             (None, 126, 126, 62, 64)  1792      \n",
            "                                                                 \n",
            " max_pooling3d (MaxPooling3D  (None, 63, 63, 31, 64)   0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 63, 63, 31, 64)   256       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " conv3d_1 (Conv3D)           (None, 61, 61, 29, 64)    110656    \n",
            "                                                                 \n",
            " max_pooling3d_1 (MaxPooling  (None, 30, 30, 14, 64)   0         \n",
            " 3D)                                                             \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 30, 30, 14, 64)   256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv3d_2 (Conv3D)           (None, 28, 28, 12, 128)   221312    \n",
            "                                                                 \n",
            " max_pooling3d_2 (MaxPooling  (None, 14, 14, 6, 128)   0         \n",
            " 3D)                                                             \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 14, 14, 6, 128)   512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv3d_3 (Conv3D)           (None, 12, 12, 4, 256)    884992    \n",
            "                                                                 \n",
            " max_pooling3d_3 (MaxPooling  (None, 6, 6, 2, 256)     0         \n",
            " 3D)                                                             \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 6, 6, 2, 256)     1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " global_average_pooling3d (G  (None, 256)              0         \n",
            " lobalAveragePooling3D)                                          \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               131584    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 512)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 513       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,352,897\n",
            "Trainable params: 1,351,873\n",
            "Non-trainable params: 1,024\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "initial_learning_rate = 0.0001\n",
        "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True\n",
        ")\n",
        "model.compile(\n",
        "    loss=\"binary_crossentropy\",\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n",
        "    metrics=[\"acc\"],\n",
        ")\n",
        "\n",
        "save_path='/content/gdrive/MyDrive/BET_BSE_DATA/brain_mri_classification_facial_features.h5'\n",
        "checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
        "    \"/content/gdrive/MyDrive/BET_BSE_DATA/brain_mri_classification_facial_features.h5\", save_best_only=True\n",
        ")\n",
        "early_stopping_cb = keras.callbacks.EarlyStopping(monitor=\"val_acc\", patience=15)\n",
        "\n",
        "epochs = 3\n",
        "new_model = keras.models.load_model(save_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "RhXlagDdvX6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.test.gpu_device_name()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "MJLwnI2TBkpi",
        "outputId": "57f9306f-43a3-4abd-e5be-59af963f3aa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=test_dataset,\n",
        "    epochs=epochs,\n",
        "    shuffle=True,\n",
        "    verbose=1,\n",
        "    callbacks=[checkpoint_cb, early_stopping_cb],\n",
        ")"
      ],
      "metadata": {
        "id": "7RRDUbOPvlZR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "373eb755-ea8d-496b-f2f2-553badcec81f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "115/115 [==============================] - 10439s 91s/step - loss: 0.0812 - acc: 0.9815 - val_loss: 0.7449 - val_acc: 0.7506\n",
            "Epoch 2/3\n",
            " 77/115 [===================>..........] - ETA: 55:27 - loss: 0.0836 - acc: 0.9756"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yohFzDaYBcMk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}